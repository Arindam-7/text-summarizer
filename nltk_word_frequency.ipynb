{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6LLAapLThZm"
      },
      "outputs": [],
      "source": [
        "# article_text = \"\"\"Automatic text summarization has been an active field of research for many years . Several approaches have been proposed ranging from simple position and word frequency methods to learning and graph based algorithms . The advent of human generated knowledge bases like Wikipedia offer a further possibility in text summarization they can be used to understand the input text in terms of salient concepts from the knowledge base . In this paper we study a novel approach that leverages Wikipedia in conjunction with graph based ranking . Our approach is to first construct a bipartite sentence concept graph and then rank the input sentences using iterative updates on this graph . We consider several models for the bipartite graph and derive convergence properties under each model . Then we take up personalized and query focused summarization where the sentence ranks additionally depend on user interests and queries respectively . Finally we present a Wikipedia based multi document summarization algorithm . An important feature of the proposed algorithms is that they enable real time incremental summarization users can first view an initial summary and then request additional content if interested . We evaluate the performance of our proposed summarizer using the ROUGE metric and the results show that leveraging Wikipedia can significantly improve summary quality . We also present results from a user study which suggests that using incremental summarization can help in better understanding news articles .\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "N1GoLHMfT-Pb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dab1e21-cddf-4aa6-933d-9f39b06f640e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/train_ts.csv')"
      ],
      "metadata": {
        "id": "VPIBWa4-Gtac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "import csv\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "  article_text = row['Abstract'].lower()\n",
        "  # remove spaces, punctuations and numbers\n",
        "  clean_text = re.sub('[^a-zA-Z]', ' ', article_text)\n",
        "  clean_text = re.sub('\\s+', ' ', clean_text)\n",
        "  # split into sentence list\n",
        "  sentence_list = nltk.sent_tokenize(article_text)\n",
        "  # word frequencies\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "  word_frequencies = {}\n",
        "  for word in nltk.word_tokenize(clean_text):\n",
        "    if word not in stopwords:\n",
        "      if word not in word_frequencies:\n",
        "        word_frequencies[word] = 1\n",
        "      else:\n",
        "        word_frequencies[word] += 1\n",
        "\n",
        "  # maximum frequency\n",
        "  maximum_frequency = max(word_frequencies.values())\n",
        "\n",
        "  for word in word_frequencies:\n",
        "    word_frequencies[word] = word_frequencies[word] / maximum_frequency\n",
        "\n",
        "  # sentence score\n",
        "  sentence_scores = {}\n",
        "\n",
        "  for sentence in sentence_list:\n",
        "    for word in nltk.word_tokenize(sentence):\n",
        "      if word in word_frequencies and len(sentence.split(' ')) < 30:\n",
        "        if sentence not in sentence_scores:\n",
        "          sentence_scores[sentence] = word_frequencies[word]\n",
        "        else:\n",
        "          sentence_scores[sentence] += word_frequencies[word]\n",
        "\n",
        "  # get top 5 sentences\n",
        "  summary = heapq.nlargest(1 , sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "  output = \" \".join(summary)\n",
        "\n",
        "  with open('output.csv', 'a') as f:\n",
        "    f.write(str(index + 1) + ') ' + output + '\\n')\n",
        "  # print(str(index + 1) + ') ' + output + '\\n')\n"
      ],
      "metadata": {
        "id": "d6ZnocmOIVHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIpdcOSnR65C",
        "outputId": "dab6b4c6-97cf-4b8b-ea82-fe0f650754a1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.21.6)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->rouge-score) (7.1.2)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=12328870d139fefc505348dfb4206ca81e16d18cda4c07c58d6fccd557baeef6\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ac/6b/38096e3c5bf1dc87911e3585875e21a3ac610348e740409c76\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "a = 'ncappc performs i NCA and ii simulation based posterior predictive checks using NCA metrics. ncappc package is highly flexible and comprehensive. Can perform both individual and population level diagnostics. Produces output summarizing the diagnostic results including the model specific outliers. The output is easy to interpret and to use in evaluation of a population PK model.'\n",
        "b = 'we developed a new package in r called ncappc to perform a nca and simulation based posterior predictive checks for a population pk model using nca metrics .'\n",
        "scores = scorer.score(a, b)"
      ],
      "metadata": {
        "id": "jK4sX-_GSC-i"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlqV1X09TCPW",
        "outputId": "bedb7af2-3a8a-49ea-d81d-4ed7816872bc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': Score(precision=0.7037037037037037, recall=0.3392857142857143, fmeasure=0.4578313253012048),\n",
              " 'rougeL': Score(precision=0.48148148148148145, recall=0.23214285714285715, fmeasure=0.3132530120481928)}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}